{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.导入数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n",
      "WARNING:theano.configdefaults:g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n",
      "Building prefix dict from the default dictionary ...\n",
      "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "DEBUG:jieba:Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.315 seconds.\n",
      "DEBUG:jieba:Loading model cost 0.315 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "DEBUG:jieba:Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "from NLP import comment_analysis as ca\n",
    "from gensim import corpora, models\n",
    "\n",
    "\n",
    "doc = ca.prepare_news_to_corpora(\"news540\")\n",
    "doc_tokens = doc[\"doc_bodies\"]\n",
    "dictionary = corpora.Dictionary(doc_tokens) \n",
    "\n",
    "corpus = [dictionary.doc2bow(text) for text in doc_tokens]\n",
    "tags = doc[\"tags\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.导入需要的模型库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "#from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.定义函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dist(x,y):\n",
    "    return np.sqrt(np.sum((x-y)**2))\n",
    "\n",
    "def lda_corpus(corpus, n_topics =50):\n",
    "    tfidf = models.TfidfModel(corpus)\n",
    "    tfidf_corpus = tfidf[corpus]    \n",
    "    lda = models.LdaMulticore(tfidf_corpus, num_topics=n_topics, passes=50, iterations=50, id2word=dictionary)\n",
    "    lda.save(\"result\\lda_tmp\")\n",
    "    aa = lda[tfidf_corpus] \n",
    "    return aa\n",
    "\n",
    "def load_lda_corpus(corpus, lda_corpus_doc, n_topics =50):\n",
    "    tfidf = models.TfidfModel(corpus)\n",
    "    tfidf_corpus = tfidf[corpus]\n",
    "    lda = models.LdaMulticore.load(lda_corpus_doc)\n",
    "    aa = lda[tfidf_corpus] \n",
    "    return aa    \n",
    "\n",
    "def lda2list(corpus):\n",
    "    #aa = lda_corpus(corpus)\n",
    "    aa = load_lda_corpus(corpus, \"result/lda_model_caicai_tfidf_50\")\n",
    "    s_l = []\n",
    "    for ll in aa:\n",
    "        s_l.append(ll)\n",
    "    s_m = ca.list2SP(s_l)\n",
    "    data = scale(s_m, with_mean=False) \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def kmeans_predict(corpus, n_clusters =10, n_pca =5):\n",
    "    data = lda2list(corpus)\n",
    "    reduced_data = PCA(n_components=n_pca).fit_transform(data.toarray())\n",
    "    kmeans = KMeans(init='k-means++', n_clusters=n_clusters, n_init=10)\n",
    "    kmeans.fit(reduced_data)\n",
    "    clus_pred = kmeans.predict(reduced_data)\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    return [clus_pred,centroids]\n",
    "\n",
    "def pred2dict(corpus):\n",
    "    clus_pred = kmeans_predict(corpus)[0]\n",
    "    k_s = {}\n",
    "    for i, p in enumerate(clus_pred):\n",
    "        if p in k_s:\n",
    "            k_s[p].append(i)\n",
    "        else:\n",
    "            k_s[p] = []\n",
    "            k_s[p].append(i)\n",
    "    return k_s    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.结果输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 445, [1, 9, 11, 12, 14, 22, 29, 32, 36, 52, 58, 68, 69, 71, 72, 81, 101, 102, 105, 128, 132, 136, 140, 162, 173, 185, 186, 187, 193, 197, 199, 200, 208, 216, 223, 224, 225, 229, 233, 234, 240, 242, 245, 250, 263, 264, 266, 269, 270, 271, 272, 274, 275, 277, 278, 279, 281, 284, 286, 289, 294, 298, 305, 312, 314, 315, 316, 318, 326, 334, 338, 344, 346, 347, 352, 364, 369, 371, 372, 376, 382, 386, 389, 391, 404, 405, 407, 411, 419, 420, 424, 426, 428, 430, 433, 437, 442, 445, 447, 450, 453, 459, 467, 469, 478, 479, 480, 488, 490, 496, 502, 503, 509, 514, 516, 517, 521, 523, 526, 532, 534, 535, 539]], [1, 356, [16, 44, 50, 59, 67, 70, 80, 91, 93, 152, 195, 198, 214, 273, 325, 330, 332, 335, 339, 345, 354, 356, 368, 392, 406, 409, 508, 527]], [2, 65, [0, 5, 6, 10, 33, 39, 53, 55, 56, 57, 61, 62, 65, 73, 100, 108, 113, 116, 119, 120, 123, 124, 125, 126, 127, 135, 141, 144, 147, 168, 172, 178, 181, 184, 227, 237, 243, 248, 251, 260, 268, 300, 302, 327, 328, 337, 357, 358, 359, 370, 380, 381, 388, 408, 436, 446, 455, 458, 465, 470, 474, 486, 487, 489, 494, 498, 506, 511, 518, 524, 530, 531]], [3, 151, [35, 37, 41, 63, 76, 87, 106, 129, 151, 166, 177, 188, 211, 215, 232, 235, 241, 249, 255, 293, 317, 320, 322, 331, 333, 378, 384, 400, 422, 427, 432, 443, 510, 520, 522, 525, 528]], [4, 23, [4, 23, 30, 64, 84, 94, 112, 175, 192, 220, 226, 228, 230, 247, 285, 288, 329, 403, 456, 461, 472, 475, 538]], [5, 171, [47, 51, 60, 86, 103, 109, 157, 164, 171, 183, 189, 194, 196, 261, 290, 291, 299, 363, 374, 379, 421, 429, 439, 466, 473, 476, 504, 512, 519, 533, 536]], [6, 416, [31, 54, 98, 139, 167, 174, 180, 204, 205, 206, 209, 217, 239, 259, 276, 283, 287, 296, 311, 360, 375, 383, 387, 397, 398, 416, 417, 451, 452, 457, 464, 468]], [7, 282, [13, 15, 17, 18, 19, 21, 25, 26, 40, 45, 83, 110, 114, 117, 149, 244, 252, 280, 282, 292, 297, 342, 390, 396, 415, 418, 435, 460, 462, 463, 485, 495, 500, 505, 529]], [8, 78, [2, 3, 7, 8, 20, 24, 27, 28, 34, 38, 42, 43, 48, 66, 74, 75, 77, 78, 79, 82, 85, 88, 89, 95, 97, 99, 104, 107, 111, 121, 122, 130, 131, 133, 134, 137, 138, 142, 143, 146, 148, 150, 153, 156, 159, 160, 161, 163, 165, 169, 170, 176, 179, 182, 190, 191, 203, 207, 210, 212, 213, 218, 219, 221, 231, 236, 238, 246, 253, 256, 262, 265, 267, 295, 301, 303, 306, 308, 310, 313, 319, 321, 323, 324, 336, 341, 343, 348, 349, 350, 361, 362, 365, 366, 367, 373, 377, 385, 393, 394, 395, 399, 401, 402, 412, 413, 423, 425, 434, 438, 441, 444, 448, 449, 454, 471, 477, 482, 484, 491, 492, 493, 497, 499, 501, 507, 513, 515, 537]], [9, 49, [46, 49, 90, 92, 96, 115, 118, 145, 154, 155, 158, 201, 202, 222, 254, 257, 258, 304, 307, 309, 340, 351, 353, 355, 410, 414, 431, 440, 481, 483]]]\n"
     ]
    }
   ],
   "source": [
    "def select_closest_corpus(corpus, n_pca =5):\n",
    "    centroids = kmeans_predict(corpus)[1]\n",
    "    k_s = pred2dict(corpus)\n",
    "    data = lda2list(corpus)\n",
    "    reduced_data = PCA(n_components=n_pca).fit_transform(data.toarray())\n",
    "    result = []\n",
    "    for i , cent in enumerate(centroids):\n",
    "        ll = k_s[i]\n",
    "        dis = []\n",
    "        ind = []\n",
    "        for index in ll:\n",
    "            dis_index = dist(cent,reduced_data[index])\n",
    "            dis.append(dis_index)\n",
    "            ind.append(index)\n",
    "        dis_min = np.min(dis)\n",
    "        index_min = ll[dis.index(dis_min)]\n",
    "        result.append([i,index_min,ll])\n",
    "    \n",
    "    return result\n",
    "    \n",
    "print select_closest_corpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_kmeans_result_doc(corpus,result_doc):\n",
    "    \n",
    "    f = open(\"news540\")\n",
    "    news = []\n",
    "\n",
    "    for line in f:\n",
    "        news.append(line)\n",
    "\n",
    "    f.close()\n",
    "\n",
    "    g = open(result_doc,\"w\")\n",
    "    \n",
    "    k_s = pred2dict(corpus)\n",
    "        \n",
    "    for k in k_s:\n",
    "        k_l = k_s[k]\n",
    "        for ll in k_l:\n",
    "            g.write((\"%s||%s\\n\") % (news[ll], k))\n",
    "    \n",
    "        g.write((\"----End of the %d cluster---------------------------------------------\\n\") % k)\n",
    "        g.write((\"----------------------------------------------------------------------\\n\") % k)\n",
    "\n",
    "    g.flush()\n",
    "    g.close() \n",
    "    \n",
    "write_kmeans_result_doc(corpus,\"result\\test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
